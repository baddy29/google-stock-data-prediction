# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n2Uzy8z7XXcuOh47H8PkytaLIDM5mkWC
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, dropout_p=0.1):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.h2o = nn.Linear(hidden_size, output_size)
        self.activation = nn.Tanh()
        self.output_activation = nn.Identity()
        self.dropout = nn.Dropout(p=dropout_p)  # Dropout layer with dropout probability

    def forward(self, input, hidden):
        combined = torch.cat((input, hidden), dim=1)
        hidden = self.activation(self.i2h(combined))
        hidden = self.dropout(hidden)  # Apply dropout to the hidden state
        output = self.h2o(hidden)
        output = self.output_activation(output)
        return output, hidden

    def initHidden(self, batch_size=1):
        return torch.ones(batch_size, self.hidden_size) * 0.01

# Define hyperparameters
input_size = 5  # Number of input features (Open, High, Low, Close, Volume)
hidden_size = 16
output_size = 1
learning_rate = 0.0001  # Adjust learning rate based on your model's convergence
num_epochs = 30
batch_size = 32
seq_length = 8  # Adjust based on the number of historical steps to consider
bptt_steps = 4  # Number of time steps for truncated BPTT

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset

class StockPriceDataset(Dataset):
    def __init__(self, csv_file, seq_length):
        """
        Initialize the StockPriceDataset.

        Args:
            csv_file (str): Path to the CSV file containing the stock price data.
            seq_length (int): Length of input sequences (number of historical prices).
        """
        # Load the CSV file
        self.data = pd.read_csv(csv_file)

        # Convert 'Date' column to datetime
        self.data['Date'] = pd.to_datetime(self.data['Date'])

        # Sort data by date (ascending order)
        self.data = self.data.sort_values('Date')

        # Remove commas and convert columns to numeric
        numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
        self.data[numeric_cols] = self.data[numeric_cols].replace({',': ''}, regex=True)
        self.data[numeric_cols] = self.data[numeric_cols].apply(pd.to_numeric, errors='coerce')

        # Normalize 'Close' column (stock prices)
        self.close_min = self.data['Close'].min()
        self.close_max = self.data['Close'].max()
        self.data['Close'] = (self.data['Close'] - self.close_min) / (self.close_max - self.close_min + 1e-10)

        # Normalize 'Open', 'High', 'Low', 'Volume' if needed
        # Example normalization for 'Volume' column
        self.volume_min = self.data['Volume'].min()
        self.volume_max = self.data['Volume'].max()
        self.data['Volume'] = (self.data['Volume'] - self.volume_min) / (self.volume_max - self.volume_min + 1e-10)

        # Length of sequences
        self.seq_length = seq_length

    def __len__(self):
        return len(self.data) - self.seq_length

    def __getitem__(self, idx):
        """
        Get a single item (input sequence and target) from the dataset.

        Args:
            idx (int): Index of the item in the dataset.

        Returns:
            tuple: Tuple containing the input sequence and target.
        """
        # Get input sequence and target
        input_seq = self.data.iloc[idx:idx+self.seq_length][['Open', 'High', 'Low', 'Close', 'Volume']].values
        target = self.data.iloc[idx+self.seq_length]['Close']

        # Convert numpy arrays to PyTorch tensors
        input_seq = torch.tensor(input_seq, dtype=torch.float32)
        target = torch.tensor(target, dtype=torch.float32)

        return input_seq, target

# Training dataset
csv_file = 'sample_data/data/Google_Stock_Price_Train.csv'
train_dataset = StockPriceDataset(csv_file, seq_length)

# Split into train/validation datasets
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Initialize model, loss function, and optimizer
rnn_model = RNN(input_size, hidden_size, output_size)
# criterion = nn.L1Loss()  # Use Mean Squared Error for regression
# criterion = nn.MSELoss()  # Use Mean Squared Error for regression
criterion = nn.HuberLoss()
optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=0.00001)

# Training loop
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    # Training phase
    rnn_model.train()
    running_train_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        hidden = rnn_model.initHidden(batch_size=inputs.size(0))
        for i in range(seq_length):
            input_step = inputs[:, i, :]  # Shape: (batch_size, input_size)
            output, hidden = rnn_model(input_step, hidden)

        loss = criterion(output.squeeze(), labels)
        loss.backward()
        optimizer.step()
        running_train_loss += loss.item()
    train_loss = running_train_loss / len(train_loader)
    train_losses.append(train_loss)

    # Validation phase
    rnn_model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            hidden = rnn_model.initHidden(batch_size=inputs.size(0))
            for i in range(seq_length):
                input_step = inputs[:, i, :]  # Shape: (batch_size, input_size)
                output, hidden = rnn_model(input_step, hidden)
            loss = criterion(output.squeeze(), labels)
            running_val_loss += loss.item()
    val_loss = running_val_loss / len(val_loader)
    val_losses.append(val_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

print("Training finished!")

# Plotting the loss graph
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Mean Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

# Training loop
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    # Training phase
    rnn_model.train()
    running_train_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        hidden = rnn_model.initHidden(batch_size=inputs.size(0))
        for i in range(seq_length):
            input_step = inputs[:, i, :]  # Shape: (batch_size, input_size)
            output, hidden = rnn_model(input_step, hidden)

        loss = criterion(output.squeeze(), labels)
        loss.backward()
        optimizer.step()
        running_train_loss += loss.item()
    train_loss = running_train_loss / len(train_loader)
    train_losses.append(train_loss)

    # Validation phase
    rnn_model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            hidden = rnn_model.initHidden(batch_size=inputs.size(0))
            for i in range(seq_length):
                input_step = inputs[:, i, :]  # Shape: (batch_size, input_size)
                output, hidden = rnn_model(input_step, hidden)
            loss = criterion(output.squeeze(), labels)
            running_val_loss += loss.item()
    val_loss = running_val_loss / len(val_loader)
    val_losses.append(val_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

print("Training finished!")

# Plotting the loss graph
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('L1 Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

# Training loop
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    # Training phase
    rnn_model.train()
    running_train_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        hidden = rnn_model.initHidden(batch_size=inputs.size(0))
        for i in range(seq_length):
            input_step = inputs[:, i, :]  # Shape: (batch_size, input_size)
            output, hidden = rnn_model(input_step, hidden)

        loss = criterion(output.squeeze(), labels)
        loss.backward()
        optimizer.step()
        running_train_loss += loss.item()
    train_loss = running_train_loss / len(train_loader)
    train_losses.append(train_loss)

    # Validation phase
    rnn_model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            hidden = rnn_model.initHidden(batch_size=inputs.size(0))
            for i in range(seq_length):
                input_step = inputs[:, i, :]  # Shape: (batch_size, input_size)
                output, hidden = rnn_model(input_step, hidden)
            loss = criterion(output.squeeze(), labels)
            running_val_loss += loss.item()
    val_loss = running_val_loss / len(val_loader)
    val_losses.append(val_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

print("Training finished!")

# Plotting the loss graph
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Huber Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

# Load dataset and create DataLoader
csv_file = 'sample_data/data/Google_Stock_Price_Test.csv'
test_dataset = StockPriceDataset(csv_file, seq_length)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Set batch_size to 1 for inference

rnn_model.eval()
# Initialize lists to store predicted and actual closing prices
predicted_prices = []
actual_prices = []

# Iterate through the test dataset for inference
with torch.no_grad():
    for inputs, labels in test_loader:
        hidden = rnn_model.initHidden(batch_size=inputs.size(0))
        # Forward pass to get predictions
        outputs = rnn_model(inputs,hidden)

        # Extract predicted closing price (output) and actual closing price (label)
        predicted_price = outputs.item()  # Assuming batch_size is 1
        actual_price = labels.item()  # Assuming batch_size is 1

        # Store predicted and actual closing prices
        predicted_prices.append(predicted_price)
        actual_prices.append(actual_price)

# Convert lists to numpy arrays for plotting
predicted_prices = np.array(predicted_prices)
actual_prices = np.array(actual_prices)


# Plot predicted vs actual closing prices
plt.figure(figsize=(10, 6))
plt.plot(actual_prices, label='Actual Closing Prices', color='blue')
plt.plot(predicted_prices, label='Predicted Closing Prices', color='red', linestyle='--')
plt.xlabel('Time')
plt.ylabel('Closing Price')
plt.title('Predicted vs Actual Closing Prices')
plt.legend()
plt.grid(True)
plt.show()