# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t7Wt3ilawd8g6HJzpgPWPtcrdROg7xza
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

# Define hyperparameters
input_size = 4 # Number of input features (Open, High, Low, Close, Volume)
hidden_size = 32
output_size = 1
learning_rate = 0.001  # Adjust learning rate based on your model's convergence
num_epochs = 30
batch_size = 32
seq_length = 8  # Adjust based on the number of historical steps to consider
bptt_steps = 4  # Number of time steps for truncated BPTT

import torch.nn.functional as F

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):
        super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)  # Add dropout layer

    def forward(self, x):
        # Input x shape: (batch_size, seq_length, input_size)
        out, _ = self.rnn(x)  # out shape: (batch_size, seq_length, hidden_size)

        # Apply dropout to the last output in the sequence
        out = self.dropout(out)

        # Use the last time step output for prediction
        out = self.fc(out[:, -1, :])  # out shape: (batch_size, output_size)
        return out

    def initHidden(self, batch_size=1):
        return torch.zeros(1, batch_size, self.hidden_size)  # Initialize hidden state for RNN

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset

class StockPriceDataset(Dataset):
    def __init__(self, csv_file, seq_length):
        """
        Initialize the StockPriceDataset.

        Args:
            csv_file (str): Path to the CSV file containing the stock price data.
            seq_length (int): Length of input sequences (number of historical prices).
        """
        # Load the CSV file
        self.data = pd.read_csv(csv_file)

        # Convert 'Date' column to datetime
        self.data['Date'] = pd.to_datetime(self.data['Date'])

        # Sort data by date (ascending order)
        self.data = self.data.sort_values('Date')

        # Remove commas and convert columns to numeric
        numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
        self.data[numeric_cols] = self.data[numeric_cols].replace({',': ''}, regex=True)
        self.data[numeric_cols] = self.data[numeric_cols].apply(pd.to_numeric, errors='coerce')

        # Normalize 'Close' column (stock prices)
        self.close_min = self.data['Close'].min()
        self.close_max = self.data['Close'].max()
        self.data['Close'] = (self.data['Close'] - self.close_min) / (self.close_max - self.close_min + 1e-10)

        # Normalize 'Open', 'High', 'Low', 'Volume' if needed
        # Example normalization for 'Volume' column
        self.volume_min = self.data['Volume'].min()
        self.volume_max = self.data['Volume'].max()
        self.data['Volume'] = (self.data['Volume'] - self.volume_min) / (self.volume_max - self.volume_min + 1e-10)

        # Length of sequences
        self.seq_length = seq_length

    def __len__(self):
        return len(self.data) - self.seq_length

    def __getitem__(self, idx):
        """
        Get a single item (input sequence and target) from the dataset.

        Args:
            idx (int): Index of the item in the dataset.

        Returns:
            tuple: Tuple containing the input sequence and target.
        """
        # Get input sequence and target
        input_seq = self.data.iloc[idx:idx+self.seq_length][['Open', 'High', 'Low', 'Volume']].values
        target = self.data.iloc[idx+self.seq_length]['Close']

        # Convert numpy arrays to PyTorch tensors
        input_seq = torch.tensor(input_seq, dtype=torch.float32)
        target = torch.tensor(target, dtype=torch.float32)

        return input_seq, target

# Create train dataset and data loader
csv_file = 'sample_data/data/Google_Stock_Price_Train.csv'  # Specify the path to your CSV file
seq_length = 8  # Length of input sequences

# Create dataset and split into train/validation
dataset = StockPriceDataset(csv_file, seq_length)
# print("dataset", dataset.__getitem__(3))
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
# print("val_dataset", val_dataset.__getitem__(1))

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Initialize model, loss function, and optimizer
rnn_model = SimpleRNN(input_size, hidden_size, output_size)
# criterion = nn.MSELoss()  # Use Mean Squared Error for regression
criterion = nn.HuberLoss(reduction='mean', delta=0.158)
# criterion = nn.L1Loss()
optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=0.001)

# Training loop
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    # Training phase
    rnn_model.train()
    running_train_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()

        # Forward pass
        outputs = rnn_model(inputs)

        # Calculate loss
        loss = criterion(outputs.squeeze(), labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item()

    # Calculate average training loss for the epoch
    train_loss = running_train_loss / len(train_loader)
    train_losses.append(train_loss)

    # Validation phase
    rnn_model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            # Forward pass
            outputs = rnn_model(inputs)

            # Calculate loss
            loss = criterion(outputs.squeeze(), labels)
            running_val_loss += loss.item()

    # Calculate average validation loss for the epoch
    val_loss = running_val_loss / len(val_loader)
    val_losses.append(val_loss)

    # Print training progress
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

# Plotting the training and validation loss over epochs
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('L1 Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

# Save the trained model (if needed)
torch.save(rnn_model.state_dict(), 'sample_data/simple_rnn_model_miso.pth')

# Training loop
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    # Training phase
    rnn_model.train()
    running_train_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()

        # Forward pass
        outputs = rnn_model(inputs)

        # Calculate loss
        loss = criterion(outputs.squeeze(), labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item()

    # Calculate average training loss for the epoch
    train_loss = running_train_loss / len(train_loader)
    train_losses.append(train_loss)

    # Validation phase
    rnn_model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            # Forward pass
            outputs = rnn_model(inputs)

            # Calculate loss
            loss = criterion(outputs.squeeze(), labels)
            running_val_loss += loss.item()

    # Calculate average validation loss for the epoch
    val_loss = running_val_loss / len(val_loader)
    val_losses.append(val_loss)

    # Print training progress
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

# Plotting the training and validation loss over epochs
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('MSE Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

# Save the trained model (if needed)
torch.save(rnn_model.state_dict(), 'sample_data/simple_rnn_model_miso.pth')

# Training loop
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    # Training phase
    rnn_model.train()
    running_train_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()

        # Forward pass
        outputs = rnn_model(inputs)

        # Calculate loss
        loss = criterion(outputs.squeeze(), labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item()

    # Calculate average training loss for the epoch
    train_loss = running_train_loss / len(train_loader)
    train_losses.append(train_loss)

    # Validation phase
    rnn_model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            # Forward pass
            outputs = rnn_model(inputs)

            # Calculate loss
            loss = criterion(outputs.squeeze(), labels)
            running_val_loss += loss.item()

    # Calculate average validation loss for the epoch
    val_loss = running_val_loss / len(val_loader)
    val_losses.append(val_loss)

    # Print training progress
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

# Plotting the training and validation loss over epochs
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Huber Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

# Save the trained model (if needed)
torch.save(rnn_model.state_dict(), 'sample_data/simple_rnn_model_miso.pth')

# Load dataset and create DataLoader
csv_file = 'sample_data/data/Google_Stock_Price_Test.csv'
test_dataset = StockPriceDataset(csv_file, seq_length)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Set batch_size to 1 for inference

rnn_model.eval()
# Initialize lists to store predicted and actual closing prices
predicted_prices = []
actual_prices = []

# Iterate through the test dataset for inference
with torch.no_grad():
    for inputs, labels in test_loader:
        # Forward pass to get predictions
        outputs = rnn_model(inputs)

        # Extract predicted closing price (output) and actual closing price (label)
        predicted_price = outputs.item()  # Assuming batch_size is 1
        actual_price = labels.item()  # Assuming batch_size is 1

        # Store predicted and actual closing prices
        predicted_prices.append(predicted_price)
        actual_prices.append(actual_price)

# Convert lists to numpy arrays for plotting
predicted_prices = np.array(predicted_prices)
actual_prices = np.array(actual_prices)


# Plot predicted vs actual closing prices
plt.figure(figsize=(10, 6))
plt.plot(actual_prices, label='Actual Closing Prices', color='blue')
plt.plot(predicted_prices, label='Predicted Closing Prices', color='red', linestyle='--')
plt.xlabel('Time')
plt.ylabel('Closing Price')
plt.title('Predicted vs Actual Closing Prices')
plt.legend()
plt.grid(True)
plt.show()