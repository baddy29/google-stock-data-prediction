# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WlBKkkkjaSGzTRMsWxRaBQYrzcmOq71g
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn

class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.1):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout

        # Define the LSTM layer
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=self.dropout if num_layers > 1 else 0)

        # Define the fully connected layer
        self.fc = nn.Linear(hidden_size, output_size)

        # Define the dropout layer (if dropout > 0 and num_layers > 1)
        self.dropout_layer = nn.Dropout(self.dropout) if self.dropout > 0 and self.num_layers > 1 else None

    def forward(self, x):
        # Initialize hidden state and cell state
        batch_size = x.size(0)
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)

        # Apply dropout if applicable
        if self.dropout_layer is not None:
            out = self.dropout_layer(out)

        # Decode the hidden state of the last time step
        out = self.fc(out[:, -1, :])  # out: tensor of shape (batch_size, output_size)
        return out

# Define hyperparameters
input_size = 5  # Number of input features (Open, High, Low, Close, Volume)
hidden_size = 16
output_size = 1
learning_rate = 0.0001
num_epochs = 30
batch_size = 32
seq_length = 8
num_layers = 2

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset

class StockPriceDataset(Dataset):
    def __init__(self, csv_file, seq_length):
        """
        Initialize the StockPriceDataset.

        Args:
            csv_file (str): Path to the CSV file containing the stock price data.
            seq_length (int): Length of input sequences (number of historical prices).
        """
        # Load the CSV file
        self.data = pd.read_csv(csv_file)

        # Convert 'Date' column to datetime
        self.data['Date'] = pd.to_datetime(self.data['Date'])

        # Sort data by date (ascending order)
        self.data = self.data.sort_values('Date')

        # Remove commas and convert columns to numeric
        numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
        self.data[numeric_cols] = self.data[numeric_cols].replace({',': ''}, regex=True)
        self.data[numeric_cols] = self.data[numeric_cols].apply(pd.to_numeric, errors='coerce')

        # Normalize 'Close' column (stock prices)
        self.close_min = self.data['Close'].min()
        self.close_max = self.data['Close'].max()
        self.data['Close'] = (self.data['Close'] - self.close_min) / (self.close_max - self.close_min + 1e-10)

        # Normalize 'Open', 'High', 'Low', 'Volume' if needed
        # Example normalization for 'Volume' column
        self.volume_min = self.data['Volume'].min()
        self.volume_max = self.data['Volume'].max()
        self.data['Volume'] = (self.data['Volume'] - self.volume_min) / (self.volume_max - self.volume_min + 1e-10)

        # Length of sequences
        self.seq_length = seq_length

    def __len__(self):
        return len(self.data) - self.seq_length

    def __getitem__(self, idx):
        """
        Get a single item (input sequence and target) from the dataset.

        Args:
            idx (int): Index of the item in the dataset.

        Returns:
            tuple: Tuple containing the input sequence and target.
        """
        # Get input sequence and target
        input_seq = self.data.iloc[idx:idx+self.seq_length][['Open', 'High', 'Low', 'Close', 'Volume']].values
        target = self.data.iloc[idx+self.seq_length]['Close']

        # Convert numpy arrays to PyTorch tensors
        input_seq = torch.tensor(input_seq, dtype=torch.float32)
        target = torch.tensor(target, dtype=torch.float32)

        return input_seq, target

# Load dataset and create DataLoader
csv_file = 'sample_data/data/Google_Stock_Price_Train.csv'
dataset = StockPriceDataset(csv_file, seq_length)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
print(len(dataset))
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Initialize LSTM model, loss function, and optimizer
model = LSTM(input_size, hidden_size, num_layers, output_size)
# criterion = nn.MSELoss()
# criterion = nn.L1Loss()
criterion = nn.HuberLoss(reduction='mean',delta=0.178)
optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999),weight_decay=0.00001)

# Training loop
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()

        # Ensure input dimensions match (batch_size, seq_length, input_size)
        assert inputs.size(1) == seq_length and inputs.size(2) == input_size

        # Forward pass
        outputs = model(inputs)

        # Compute loss
        loss = criterion(outputs.squeeze(), labels)
        train_loss += loss.item()

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    # Compute average training loss
    train_loss /= len(train_loader)
    train_losses.append(train_loss)

    # Validation
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            # Ensure input dimensions match (batch_size, seq_length, input_size)
            assert inputs.size(1) == seq_length and inputs.size(2) == input_size

            # Forward pass
            outputs = model(inputs)

            # Compute loss
            val_loss += criterion(outputs.squeeze(), labels).item()

    # Compute average validation loss
    val_loss /= len(val_loader)
    val_losses.append(val_loss)

    # Print epoch statistics
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

# Plot training and validation losses
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('L1 Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

print("Training finished!")

# Training loop
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()

        # Ensure input dimensions match (batch_size, seq_length, input_size)
        assert inputs.size(1) == seq_length and inputs.size(2) == input_size

        # Forward pass
        outputs = model(inputs)

        # Compute loss
        loss = criterion(outputs.squeeze(), labels)
        train_loss += loss.item()

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    # Compute average training loss
    train_loss /= len(train_loader)
    train_losses.append(train_loss)

    # Validation
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            # Ensure input dimensions match (batch_size, seq_length, input_size)
            assert inputs.size(1) == seq_length and inputs.size(2) == input_size

            # Forward pass
            outputs = model(inputs)

            # Compute loss
            val_loss += criterion(outputs.squeeze(), labels).item()

    # Compute average validation loss
    val_loss /= len(val_loader)
    val_losses.append(val_loss)

    # Print epoch statistics
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

# Plot training and validation losses
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('MSE Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

print("Training finished!")

# Training loop
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()

        # Ensure input dimensions match (batch_size, seq_length, input_size)
        assert inputs.size(1) == seq_length and inputs.size(2) == input_size

        # Forward pass
        outputs = model(inputs)

        # Compute loss
        loss = criterion(outputs.squeeze(), labels)
        train_loss += loss.item()

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    # Compute average training loss
    train_loss /= len(train_loader)
    train_losses.append(train_loss)

    # Validation
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            # Ensure input dimensions match (batch_size, seq_length, input_size)
            assert inputs.size(1) == seq_length and inputs.size(2) == input_size

            # Forward pass
            outputs = model(inputs)

            # Compute loss
            val_loss += criterion(outputs.squeeze(), labels).item()

    # Compute average validation loss
    val_loss /= len(val_loader)
    val_losses.append(val_loss)

    # Print epoch statistics
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

# Plot training and validation losses
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Huber Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

print("Training finished!")

# Load dataset and create DataLoader
csv_file = 'sample_data/data/Google_Stock_Price_Test.csv'
test_dataset = StockPriceDataset(csv_file, 1)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Set batch_size to 1 for inference

model.eval()
# Initialize lists to store predicted and actual closing prices
predicted_prices = []
actual_prices = []

# Iterate through the test dataset for inference
with torch.no_grad():
    for inputs, labels in test_loader:
        # Forward pass to get predictions
        outputs = model(inputs)

        # Extract predicted closing price (output) and actual closing price (label)
        predicted_price = outputs.item()  # Assuming batch_size is 1
        actual_price = labels.item()  # Assuming batch_size is 1

        # Store predicted and actual closing prices
        predicted_prices.append(predicted_price)
        actual_prices.append(actual_price)

# Convert lists to numpy arrays for plotting
predicted_prices = np.array(predicted_prices)
actual_prices = np.array(actual_prices)


# Plot predicted vs actual closing prices
plt.figure(figsize=(10, 6))
plt.plot(actual_prices, label='Actual Closing Prices', color='blue')
plt.plot(predicted_prices, label='Predicted Closing Prices', color='red', linestyle='--')
plt.xlabel('Time')
plt.ylabel('Closing Price')
plt.title('Predicted vs Actual Closing Prices')
plt.legend()
plt.grid(True)
plt.show()